{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8296a2ef-f734-4665-966f-048574824336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "951bfe9c-73d1-4ff5-92a1-373b422f61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c9ae1e3-cdfc-445f-8146-d30c22e8c0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21458488 total parameters in EfficientNetV2\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sum(p.numel() for p in model.parameters())} total parameters in EfficientNetV2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b0450ebb-e61a-4eb1-a305-aec18b5a082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = mpimg.imread(rf\"C:\\Users\\Mateo\\Desktop\\repo-projects-clone\\efficientnet-deployed\\flask-api\\images\\birds.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b6639c31-4df7-4a28-b06c-8d7252f94448",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = torch.tensor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6a0fabdf-d70b-46fa-a623-014c9f5fd0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if img_tensor.shape[-1] > 3:\n",
    "    # Discard additional channels\n",
    "    img_tensor = img_tensor[..., :3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4097f875-253c-48de-a259-e1e6d7d49dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[235, 228, 209],\n",
       "         [236, 229, 210],\n",
       "         [237, 230, 211],\n",
       "         ...,\n",
       "         [246, 241, 222],\n",
       "         [246, 241, 222],\n",
       "         [246, 241, 222]],\n",
       "\n",
       "        [[235, 228, 209],\n",
       "         [236, 229, 210],\n",
       "         [237, 230, 211],\n",
       "         ...,\n",
       "         [246, 241, 222],\n",
       "         [246, 241, 222],\n",
       "         [246, 241, 222]],\n",
       "\n",
       "        [[235, 228, 209],\n",
       "         [236, 229, 210],\n",
       "         [237, 230, 211],\n",
       "         ...,\n",
       "         [245, 240, 221],\n",
       "         [245, 240, 221],\n",
       "         [245, 240, 221]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[103,  98,  78],\n",
       "         [105, 100,  80],\n",
       "         [103,  98,  78],\n",
       "         ...,\n",
       "         [122, 108,  81],\n",
       "         [121, 110,  82],\n",
       "         [117, 106,  78]],\n",
       "\n",
       "        [[101,  96,  76],\n",
       "         [108, 103,  83],\n",
       "         [109, 104,  84],\n",
       "         ...,\n",
       "         [128, 114,  87],\n",
       "         [122, 111,  83],\n",
       "         [113, 102,  74]],\n",
       "\n",
       "        [[102,  97,  77],\n",
       "         [113, 108,  88],\n",
       "         [117, 112,  92],\n",
       "         ...,\n",
       "         [126, 112,  85],\n",
       "         [118, 107,  79],\n",
       "         [109,  98,  70]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46217b63-d7dd-4d2c-86b9-e282d7cd7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1a92d-798b-4225-bbe0-b412a59402ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_correct_shape = img_tensor.permute(2, 0, 1) # correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffff65c-7beb-4883-8b09-1163e37f8c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform used in training\n",
    "efficientnet_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((384, 384), interpolation=F.InterpolationMode.BILINEAR),\n",
    "    transforms.CenterCrop(384),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3278971a-03b9-4d49-b4c5-8b63cf8cd72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_tensor)\n",
    "plt.title(\"Sample Image Original\")\n",
    "plt.xlabel(\"Height (pixels)\")\n",
    "plt.ylabel(\"Width (pixels)\")\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5ec38-ca54-44a8-950e-6c1009c7f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = efficientnet_transforms(img_correct_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951a61b-1111-4e62-805c-de2feb3cc2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(transformed.permute(1, 2, 0), interpolation=\"nearest\")\n",
    "plt.title(\"Sample Image EfficientNet Transform\")\n",
    "plt.xlabel(\"Height (pixels)\")\n",
    "plt.ylabel(\"Width (pixels)\")\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d5f09-4760-4bfd-8141-97c79344dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a812d7af-b99e-4575-83b0-4ab0d5ca9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred = model(transformed.unsqueeze(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b0dab3-e4fd-42e1-8ad6-e821676fa6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb03674e-902a-4565-b4bf-85fe1e8e6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.sort(pred.softmax(dim=1), descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee37575-d00d-4653-87bc-4355885181a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa2d8c-b2f5-42dd-bb5e-cf7d2f0746f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = torch.Tensor.tolist(result.values * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285862b5-d178-4728-9ca2-23af65df273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = probabilities[0] # Getting rid of extra dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae395c51-717d-4eed-8235-847406d98c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_list = torch.Tensor.tolist(result.indices)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168d97b-82e5-4adf-a1e3-1582608a9fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"imagenet_class_index.json\", \"r\") as f:\n",
    "    class_dict = json.load(f)\n",
    "classes = ([value[1] for value in class_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c38f06b-4bc9-4840-87ed-7ea3a9fb1a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2881a-85f0-4d53-b3f8-079abfe6f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model is {probabilities[0]:4f}% sure image is a {classes[indices_list[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979d7393-b78e-4306-9488-050e5e8e49c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities[1], classes[indices_list[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecf6b9c-1409-4366-824a-b9c25854d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities[2], classes[indices_list[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "369fa648-f114-4dc6-a297-a78b412b85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bb410022-e185-4bc1-aa1b-e7ad1f2fa163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, transforms\n",
    "models = {\n",
    "    \"efficientnet_b0\": {\"model_func\": models.efficientnet_b0, \n",
    "                        \"weights\": models.EfficientNet_B0_Weights.DEFAULT, \n",
    "                        \"transform_info\": {\n",
    "                            \"resize_size\": 256,\n",
    "                            \"interpolation\": F.InterpolationMode.BICUBIC,\n",
    "                            \"central_crop\": 224,\n",
    "                            \"normalise_params\": [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)]\n",
    "                        }},\n",
    "    \"efficientnet_b1\": {\"model_func\": models.efficientnet_b1, \n",
    "                        \"weights\": models.EfficientNet_B1_Weights.DEFAULT, \n",
    "                        \"transform_info\": {\n",
    "                            \"resize_size\": 256,\n",
    "                            \"interpolation\": F.InterpolationMode.BICUBIC,\n",
    "                            \"central_crop\": 240,\n",
    "                            \"normalise_params\": [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)]\n",
    "                        }},\n",
    "    \"efficientnet_b2\": {\"model_func\": models.efficientnet_b2, \n",
    "                        \"weights\": models.EfficientNet_B2_Weights.DEFAULT, \n",
    "                        \"transform_info\": {\n",
    "                            \"resize_size\": 288,\n",
    "                            \"interpolation\": F.InterpolationMode.BICUBIC,\n",
    "                            \"central_crop\": 288,\n",
    "                            \"normalise_params\": [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)]\n",
    "                        }},\n",
    "    \"efficientnet_b3\": {\"model_func\": models.efficientnet_b3, \n",
    "                        \"weights\": models.EfficientNet_B3_Weights.DEFAULT, \n",
    "                        \"transform_info\": {\n",
    "                            \"resize_size\": 320,\n",
    "                            \"interpolation\": F.InterpolationMode.BICUBIC,\n",
    "                            \"central_crop\": 300,\n",
    "                            \"normalise_params\": [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)]\n",
    "                        }},\n",
    "    \"efficientnet_b4\": {\"model_func\": models.efficientnet_b4, \n",
    "                        \"weights\": models.EfficientNet_B4_Weights.DEFAULT, \n",
    "                        \"transform_info\": {\n",
    "                            \"resize_size\": 384,\n",
    "                            \"interpolation\": F.InterpolationMode.BICUBIC,\n",
    "                            \"central_crop\": 380,\n",
    "                            \"normalise_params\": [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)]\n",
    "                        }},\n",
    "    \"efficientnet_b5\": {\"model_func\": models.efficientnet_b5, \n",
    "                        \"weights\": models.EfficientNet_B5_Weights.DEFAULT, \n",
    "                        \"transform_info\": {\n",
    "                            \"resize_size\": 456,\n",
    "                            \"interpolation\": F.InterpolationMode.BICUBIC,\n",
    "                            \"central_crop\": 456,\n",
    "                            \"normalise_params\": [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)]\n",
    "                        }},\n",
    "    \"efficientnet_b6\": {\"model_func\": models.efficientnet_b6, \n",
    "                        \"weights\": models.EfficientNet_B6_Weights.DEFAULT, \n",
    "                        \"transform_info\": {\n",
    "                            \"resize_size\": 528,\n",
    "                            \"interpolation\": F.InterpolationMode.BICUBIC,\n",
    "                            \"central_crop\": 528,\n",
    "                            \"normalise_params\": [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)]\n",
    "                        }},\n",
    "    \"efficientnet_b7\": {\"model_func\": models.efficientnet_b7, \n",
    "                        \"weights\": models.EfficientNet_B7_Weights.DEFAULT, \n",
    "                        \"transform_info\": {\n",
    "                            \"resize_size\": 600,\n",
    "                            \"interpolation\": F.InterpolationMode.BICUBIC,\n",
    "                            \"central_crop\": 600,\n",
    "                            \"normalise_params\": [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)]\n",
    "                        }},\n",
    "    \"efficientnet_v2_s\": {\"model_func\": models.efficientnet_v2_s, \n",
    "                        \"weights\": models.EfficientNet_V2_S_Weights.DEFAULT, \n",
    "                        \"transform_info\": {\n",
    "                            \"resize_size\": 384,\n",
    "                            \"interpolation\": F.InterpolationMode.BILINEAR,\n",
    "                            \"central_crop\": 384,\n",
    "                            \"normalise_params\": [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)]\n",
    "                        }},\n",
    "    \"efficientnet_v2_m\": {\"model_func\": models.efficientnet_v2_m, \n",
    "                        \"weights\": models.EfficientNet_V2_M_Weights.DEFAULT, \n",
    "                        \"transform_info\": {\n",
    "                            \"resize_size\": 480,\n",
    "                            \"interpolation\": F.InterpolationMode.BILINEAR,\n",
    "                            \"central_crop\": 480,\n",
    "                            \"normalise_params\": [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)]\n",
    "                        }},\n",
    "    \"efficientnet_v2_l\": {\"model_func\": models.efficientnet_v2_l, \n",
    "                        \"weights\": models.EfficientNet_V2_L_Weights.DEFAULT, \n",
    "                        \"transform_info\": {\n",
    "                            \"resize_size\": 480,\n",
    "                            \"interpolation\": F.InterpolationMode.BICUBIC,\n",
    "                            \"central_crop\": 480,\n",
    "                            \"normalise_params\": [(0.5, 0.5, 0.5), (0.5, 0.5, 0.5)]\n",
    "                        }},\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2f0356a3-79e2-4edf-807e-69150d2cab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func = models['efficientnet_v2_l']['model_func']\n",
    "weights = models['efficientnet_v2_l']['weights']\n",
    "transform_info = models['efficientnet_v2_l']['transform_info']\n",
    "resize = transform_info['resize_size']\n",
    "interpolation = transform_info['interpolation']\n",
    "central_crop = transform_info['central_crop']\n",
    "mean, std = transform_info['normalise_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2732b0b4-0fac-4186-8a15-e669b00b0458",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1= model_func(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f0e6e8c7-bf96-43e1-86c2-8eafa1b4628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_path: str, top_n: int, model_choice: str):\n",
    "    # Defining model with pretrained weights\n",
    "    model_func = models[model_choice]['model_func']\n",
    "    weights = models[model_choice]['weights']\n",
    "    resize = models[model_choice]['transform_info']['resize_size']\n",
    "    interpolation = models[model_choice]['transform_info']['interpolation']\n",
    "    central_crop = models[model_choice]['transform_info']['central_crop']\n",
    "    mean, std = models[model_choice]['transform_info']['normalise_params']\n",
    "    \n",
    "    model = model_func(weights=weights)\n",
    "\n",
    "    image = mpimg.imread(img_path)\n",
    "    img_tensor = torch.tensor(image)\n",
    "    if img_tensor.shape[-1] > 3:\n",
    "        # Discard additional channels\n",
    "        img_tensor = img_tensor[..., :3]\n",
    "    img_correct_shape = img_tensor.permute(2, 0, 1) # correct shape\n",
    "\n",
    "    # transform used in training of efficientnet\n",
    "    efficientnet_transforms = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resize, resize), interpolation=interpolation),\n",
    "        transforms.CenterCrop(central_crop),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((mean), (std)),\n",
    "    ])\n",
    "    # transform\n",
    "    transformed = efficientnet_transforms(img_correct_shape)\n",
    "\n",
    "    model.eval()\n",
    "    pred = model(transformed.unsqueeze(dim=0)) # get prediction\n",
    "    result = torch.sort(pred.softmax(dim=1), descending=True) # sorted predictions after softmax\n",
    "    probabilities = torch.Tensor.tolist(result.values * 100)[0] # list of probabilities\n",
    "    indices_list = torch.Tensor.tolist(result.indices)[0]\n",
    "\n",
    "    # get class label dict \n",
    "    with open(\"imagenet_class_index.json\", \"r\") as f:\n",
    "        class_dict = json.load(f)\n",
    "    classes = ([value[1].replace(\"_\", \" \").title() for value in class_dict.values()])\n",
    "    \n",
    "    if top_n > 10:\n",
    "        top_n = 10\n",
    "        \n",
    "    top_preds = [classes[indices_list[i]] for i in range(top_n)]\n",
    "    top_preds_confidence = [round(probabilities[i], 2) for i in range(top_n)]\n",
    "    \n",
    "    json_keys = [f'prediction{i}' for i in range(top_n)]\n",
    "    \n",
    "    output = {key: {\"prediction\": pred, \"confidence\": conf} for key, pred, conf in zip(json_keys, top_preds, top_preds_confidence)}\n",
    "\n",
    "    return  output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "30568910-aace-47e7-baab-ba6d51a169fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to C:\\Users\\Mateo/.cache\\torch\\hub\\checkpoints\\efficientnet_b0_rwightman-7f5810bc.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 20.5M/20.5M [00:02<00:00, 10.2MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction0': {'prediction': 'Pelican', 'confidence': 46.78},\n",
       " 'prediction1': {'prediction': 'Warplane', 'confidence': 9.59},\n",
       " 'prediction2': {'prediction': 'Crane', 'confidence': 7.97}}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(rf\"C:\\Users\\Mateo\\Desktop\\repo-projects-clone\\efficientnet-deployed\\flask-api\\images\\birds.jpg\", 3, 'efficientnet_b0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "81773dd4-e1b3-496b-9c0c-da20fa27532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b1-c27df63c.pth\" to C:\\Users\\Mateo/.cache\\torch\\hub\\checkpoints\\efficientnet_b1-c27df63c.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 30.1M/30.1M [00:03<00:00, 9.46MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction0': {'prediction': 'Albatross', 'confidence': 20.52},\n",
       " 'prediction1': {'prediction': 'Pelican', 'confidence': 6.32},\n",
       " 'prediction2': {'prediction': 'American Egret', 'confidence': 5.57}}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(rf\"C:\\Users\\Mateo\\Desktop\\repo-projects-clone\\efficientnet-deployed\\flask-api\\images\\birds.jpg\", 3, 'efficientnet_b1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1cc77ecc-a9ad-4d4f-95d3-e13209b082a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b7_lukemelas-c5b4e57e.pth\" to C:\\Users\\Mateo/.cache\\torch\\hub\\checkpoints\\efficientnet_b7_lukemelas-c5b4e57e.pth\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 255M/255M [00:24<00:00, 11.1MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction0': {'prediction': 'Goose', 'confidence': 20.7},\n",
       " 'prediction1': {'prediction': 'European Gallinule', 'confidence': 12.86},\n",
       " 'prediction2': {'prediction': 'Magpie', 'confidence': 8.53}}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(rf\"C:\\Users\\Mateo\\Desktop\\repo-projects-clone\\efficientnet-deployed\\flask-api\\images\\birds.jpg\", 3, 'efficientnet_b7')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d3d6b4-6789-4339-b369-26af04bd25ef",
   "metadata": {},
   "source": [
    "# COPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a0b1a7a5-f89f-4956-af2d-6587878cb179",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 56\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  output\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 56\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage Path: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(predict(img_path))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "import json\n",
    "\n",
    "def predict(img_path: str, top_n: int):\n",
    "    # Defining model with pretrained weights\n",
    "    model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    image = mpimg.imread(img_path)\n",
    "    img_tensor = torch.tensor(image)\n",
    "    if img_tensor.shape[-1] > 3:\n",
    "        # Discard additional channels\n",
    "        img_tensor = img_tensor[..., :3]\n",
    "    img_correct_shape = img_tensor.permute(2, 0, 1) # correct shape\n",
    "\n",
    "    # transform used in training of efficientnet\n",
    "    efficientnet_transforms = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((384, 384), interpolation=F.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(384),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    # transform\n",
    "    transformed = efficientnet_transforms(img_correct_shape)\n",
    "\n",
    "    model.eval()\n",
    "    pred = model(transformed.unsqueeze(dim=0)) # get prediction\n",
    "    result = torch.sort(pred.softmax(dim=1), descending=True) # sorted predictions after softmax\n",
    "    probabilities = torch.Tensor.tolist(result.values * 100)[0] # list of probabilities\n",
    "    indices_list = torch.Tensor.tolist(result.indices)[0]\n",
    "\n",
    "    # get class label dict \n",
    "    with open(\"imagenet_class_index.json\", \"r\") as f:\n",
    "        class_dict = json.load(f)\n",
    "    classes = ([value[1].replace(\"_\", \" \").title() for value in class_dict.values()])\n",
    "    \n",
    "    if top_n > 10:\n",
    "        top_n = 10\n",
    "        \n",
    "    top_preds = [classes[indices_list[i]] for i in range(top_n)]\n",
    "    top_preds_confidence = [round(probabilities[i], 2) for i in range(top_n)]\n",
    "    \n",
    "    json_keys = [f'prediction{i}' for i in range(top_n)]\n",
    "    \n",
    "    output = {key: {\"prediction\": pred, \"confidence\": conf} for key, pred, conf in zip(json_keys, top_preds, top_preds_confidence)}\n",
    "\n",
    "    return  output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    img_path = input(\"Image Path: \")\n",
    "    print(predict(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a947ba-bd84-4c12-897c-734fa9fc256c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
